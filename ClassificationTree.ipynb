{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis notebook is oriented to use a given dataset customized to contains 6 features and training a classification tree\\nThis features are: cqt, rmse, energy, mfccs, chromagram and spectral contrast\\nThis notebook is currently in development phase, so contains a considerably quantity of test code\\n\\n'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This notebook is oriented to use a given dataset customized to contains 6 features and training a classification tree\n",
    "This features are: cqt, rmse, energy, mfccs, chromagram and spectral contrast\n",
    "This notebook is currently in development phase, so contains a considerably quantity of test code\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Genre  Energy_Mean  RMSE_Mean  ZCR_Mean  CQT_Mean  \\\n",
      "0           0  Blues    42.006336   0.130295      5498  0.024378   \n",
      "1           1  Blues    28.860627   0.112494      5840  0.006032   \n",
      "2           2  Blues    44.372582   0.131844      4781  0.012576   \n",
      "3           3  Blues    39.969162   0.132396      4643  0.014717   \n",
      "4           4  Blues    44.813606   0.143477      4699  0.035051   \n",
      "\n",
      "   Spectral_Contrast_Mean  Chromagram_Mean  Tempo_Mean  STFT_Mean  ...  \\\n",
      "0               20.876659         0.278372  129.199219   0.000074  ...   \n",
      "1               20.660786         0.275674  123.046875   0.000080  ...   \n",
      "2               20.529431         0.273605  123.046875   0.000058  ...   \n",
      "3               20.406874         0.280773  123.046875   0.000039  ...   \n",
      "4               20.370873         0.273082  123.046875   0.000046  ...   \n",
      "\n",
      "   MFCCS3  MFCCS4  MFCCS5  MFCCS6  MFCCS7  MFCCS8  MFCCS9  MFCCS10  MFCCS11  \\\n",
      "0     337     114     187      64      44      51      34       43       59   \n",
      "1     164      82      94     140      57      45      23       56       67   \n",
      "2     332     141     130      72      33      54      55       45       59   \n",
      "3     131     191      80      63      65      42      40       84       75   \n",
      "4     167     117      71     103      38      44      30       50       38   \n",
      "\n",
      "   MFCCS12  \n",
      "0       29  \n",
      "1       25  \n",
      "2       23  \n",
      "3       25  \n",
      "4       36  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import librosa, librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Reading Audio Data\n",
    "\n",
    "# features = pd.read_csv('Datasets/2000Spoti.csv', encoding='latin-1', thousands=',')\n",
    "\n",
    "# features = pd.read_csv('Dataframes/var_mean_dict.csv', encoding='latin-1', thousands=',') # Media genera 0.62-0.67 con todas las caracteristicas **best result2**\n",
    "features = pd.read_csv('Dataframes/sliceDataframe/var_mean_dict_10000_Samples.csv', encoding='latin-1', thousands=',') # Media genera 0.62-0.67 con todas las caracteristicas **best result2**\n",
    "\n",
    "print(features.head(5))\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blues' 'Classical' 'Country' 'Disco' 'Hiphop' 'Jazz' 'Metal' 'Pop'\n",
      " 'Reggae' 'Rock']\n",
      "The labels of out dataset are: ['Blues' 'Classical' 'Country' 'Disco' 'Hiphop' 'Jazz' 'Metal' 'Pop'\n",
      " 'Reggae' 'Rock']\n",
      "And their shape is: (10000,)\n",
      "The features of out dataset are:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\anaconda3\\envs\\tfg\\lib\\site-packages\\ipykernel_launcher.py:45: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spectral_Contrast_Mean</th>\n",
       "      <th>Chromagram_Mean</th>\n",
       "      <th>STFT_Mean</th>\n",
       "      <th>Autocorrelation_Mean</th>\n",
       "      <th>Spectral_Centroid_Mean</th>\n",
       "      <th>Rolloff_Mean</th>\n",
       "      <th>Energy_Var</th>\n",
       "      <th>RMSE_Var</th>\n",
       "      <th>CQT_Var</th>\n",
       "      <th>Chromagram_Var</th>\n",
       "      <th>Autocorrelation_Var</th>\n",
       "      <th>Spectral_Centroid_Var</th>\n",
       "      <th>Rolloff_Var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.876659</td>\n",
       "      <td>0.278372</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>58.393989</td>\n",
       "      <td>1773.285877</td>\n",
       "      <td>3689.714355</td>\n",
       "      <td>1420.957631</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.319991</td>\n",
       "      <td>0.005842</td>\n",
       "      <td>113157.328633</td>\n",
       "      <td>168244.728448</td>\n",
       "      <td>1.084507e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.660786</td>\n",
       "      <td>0.275674</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>64.492910</td>\n",
       "      <td>1816.195860</td>\n",
       "      <td>3838.541917</td>\n",
       "      <td>405.017719</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.285368</td>\n",
       "      <td>0.007337</td>\n",
       "      <td>60033.748726</td>\n",
       "      <td>90703.325185</td>\n",
       "      <td>6.702624e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.529431</td>\n",
       "      <td>0.273605</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>51.091079</td>\n",
       "      <td>1788.642783</td>\n",
       "      <td>3955.152494</td>\n",
       "      <td>1708.942772</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.392569</td>\n",
       "      <td>0.008474</td>\n",
       "      <td>243928.138751</td>\n",
       "      <td>111322.537051</td>\n",
       "      <td>7.869501e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.406874</td>\n",
       "      <td>0.280773</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>60.225847</td>\n",
       "      <td>1654.902168</td>\n",
       "      <td>3530.617112</td>\n",
       "      <td>703.118149</td>\n",
       "      <td>0.002423</td>\n",
       "      <td>0.318879</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>111004.949290</td>\n",
       "      <td>112316.264385</td>\n",
       "      <td>9.115652e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.370873</td>\n",
       "      <td>0.273082</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>73.685011</td>\n",
       "      <td>1630.737017</td>\n",
       "      <td>3439.432279</td>\n",
       "      <td>652.843329</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.388950</td>\n",
       "      <td>0.008759</td>\n",
       "      <td>154556.712147</td>\n",
       "      <td>79648.228297</td>\n",
       "      <td>6.096879e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Spectral_Contrast_Mean  Chromagram_Mean  STFT_Mean  Autocorrelation_Mean  \\\n",
       "0               20.876659         0.278372   0.000074             58.393989   \n",
       "1               20.660786         0.275674   0.000080             64.492910   \n",
       "2               20.529431         0.273605   0.000058             51.091079   \n",
       "3               20.406874         0.280773   0.000039             60.225847   \n",
       "4               20.370873         0.273082   0.000046             73.685011   \n",
       "\n",
       "   Spectral_Centroid_Mean  Rolloff_Mean   Energy_Var  RMSE_Var   CQT_Var  \\\n",
       "0             1773.285877   3689.714355  1420.957631  0.003541  0.319991   \n",
       "1             1816.195860   3838.541917   405.017719  0.001466  0.285368   \n",
       "2             1788.642783   3955.152494  1708.942772  0.004602  0.392569   \n",
       "3             1654.902168   3530.617112   703.118149  0.002423  0.318879   \n",
       "4             1630.737017   3439.432279   652.843329  0.001724  0.388950   \n",
       "\n",
       "   Chromagram_Var  Autocorrelation_Var  Spectral_Centroid_Var   Rolloff_Var  \n",
       "0        0.005842        113157.328633          168244.728448  1.084507e+06  \n",
       "1        0.007337         60033.748726           90703.325185  6.702624e+05  \n",
       "2        0.008474        243928.138751          111322.537051  7.869501e+05  \n",
       "3        0.004500        111004.949290          112316.264385  9.115652e+05  \n",
       "4        0.008759        154556.712147           79648.228297  6.096879e+05  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#extracting labels and features\n",
    "\n",
    "labelColumn = 'Genre'\n",
    "\n",
    "labels = np.array(features[labelColumn])\n",
    "print(np.unique(labels))\n",
    "\n",
    "print(\"The labels of out dataset are:\", np.unique(labels))\n",
    "print(\"And their shape is:\", labels.shape)\n",
    "\n",
    "\n",
    "# Only mfcc\n",
    "            \n",
    "# featureColumsToDrop = ['Unnamed: 0', 'STFT_Mean.1', 'STFT_Var.1', 'Genre', 'Energy_Mean', 'RMSE_Mean', 'ZCR_Mean', 'CQT_Mean', \n",
    "#                            'Spectral_Contrast_Mean', 'Chromagram_Mean', 'Tempo_Mean', 'STFT_Mean', \n",
    "#                           'Autocorrelation_Mean', 'STFT_Mean', 'Spectral_Centroid_Mean', 'Rolloff_Mean',\n",
    "#                           'Energy_Var', 'RMSE_Var', 'ZCR_Var', 'CQT_Var', 'Spectral_Contrast_Var', 'Chromagram_Var', 'Tempo_Var', 'STFT_Var',\n",
    "#                           'Autocorrelation_Var', 'STFT_Var', 'Spectral_Centroid_Var', 'Rolloff_Var']\n",
    "\n",
    "# ALL\n",
    "\n",
    "# featureColumsToDrop = ['Unnamed: 0', 'STFT_Mean.1', 'STFT_Var.1', 'Genre']\n",
    "\n",
    "# Feture importance ordered\n",
    "\n",
    "featureColumsToDrop = ['Unnamed: 0', 'STFT_Mean.1', 'STFT_Var.1', 'Genre',\n",
    "                        'Energy_Mean', 'RMSE_Mean', 'ZCR_Mean', 'CQT_Mean', 'Tempo_Mean',\n",
    "                        'ZCR_Var', 'Spectral_Contrast_Var','Tempo_Var', 'STFT_Var',\n",
    "                        'MFCCS0','MFCCS1','MFCCS2','MFCCS3','MFCCS4','MFCCS5','MFCCS6','MFCCS7',\n",
    "                        'MFCCS8','MFCCS9','MFCCS10', 'CQT_Mean','MFCCS11','MFCCS12']\n",
    "\n",
    "# NO MFCCS mean and var\n",
    "\n",
    "# featureColumsToDrop = ['Unnamed: 0', 'STFT_Mean.1', 'STFT_Var.1', 'Genre',\n",
    "#                        'Energy_Mean', 'RMSE_Mean', 'ZCR_Mean', 'CQT_Mean', 'Spectral_Contrast_Mean', 'Chromagram_Mean', 'Tempo_Mean', 'STFT_Mean', \n",
    "#                       'Autocorrelation_Mean', 'STFT_Mean', 'Spectral_Centroid_Mean', 'Rolloff_Mean',\n",
    "#                        'MFCCS0','MFCCS1','MFCCS2','MFCCS3','MFCCS4','MFCCS5','MFCCS6','MFCCS7','MFCCS8','MFCCS9','MFCCS10', 'CQT_Mean','MFCCS11','MFCCS12', 'CQT_Var', 'STFT_Var.1', 'Genre']  \n",
    "\n",
    "\n",
    "# First prove that was made\n",
    "\n",
    "# featureColumsToDrop = ['Unnamed: 0', 'Autocorrelation_Mean', 'STFT_Mean', 'Rolloff_Mean', 'Energy_Var', 'RMSE_Var', 'ZCR_Var', 'CQT_Var', 'Spectral_Contrast_Var', 'Chromagram_Var', 'Tempo_Var', 'STFT_Var',\n",
    "#                       'Autocorrelation_Var', 'STFT_Var', 'Spectral_Centroid_Var', 'Rolloff_Var', 'CQT_Mean', 'STFT_Mean.1', 'STFT_Var.1', 'Genre']\n",
    "\n",
    "features = features.drop(featureColumsToDrop, 1)\n",
    "\n",
    "print(\"The features of out dataset are:\")\n",
    "\n",
    "features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (7500, 13)\n",
      "Training Labels Shape: (7500,)\n",
      "Testing Features Shape: (2500, 13)\n",
      "Testing Labels Shape: (2500,)\n",
      "Train Features Scaled Shape: (7500, 13)\n",
      "Testing Features Scaled Shape: (2500, 13)\n",
      "-------------------------------------------\n",
      "Metrics info section\n",
      "-------------------------------------------\n",
      "Scores by fold\n",
      "[0.806      0.818      0.80266667 0.79866667 0.81333333]\n",
      "Feature importance in the model\n",
      "-------------------------------------------\n",
      "                   feature  importance\n",
      "8                  CQT_Var    0.090742\n",
      "0   Spectral_Contrast_Mean    0.087168\n",
      "5             Rolloff_Mean    0.087079\n",
      "6               Energy_Var    0.083670\n",
      "2                STFT_Mean    0.082942\n",
      "3     Autocorrelation_Mean    0.081573\n",
      "12             Rolloff_Var    0.078053\n",
      "10     Autocorrelation_Var    0.077336\n",
      "4   Spectral_Centroid_Mean    0.077294\n",
      "7                 RMSE_Var    0.071758\n",
      "11   Spectral_Centroid_Var    0.069303\n",
      "9           Chromagram_Var    0.056660\n",
      "1          Chromagram_Mean    0.056423\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Blues       0.80      0.87      0.83       233\n",
      "   Classical       0.91      0.92      0.92       277\n",
      "     Country       0.74      0.79      0.76       253\n",
      "       Disco       0.79      0.84      0.82       241\n",
      "      Hiphop       0.86      0.81      0.83       236\n",
      "        Jazz       0.82      0.79      0.80       264\n",
      "       Metal       0.92      0.90      0.91       259\n",
      "         Pop       0.84      0.81      0.83       255\n",
      "      Reggae       0.86      0.81      0.83       242\n",
      "        Rock       0.77      0.76      0.76       240\n",
      "\n",
      "    accuracy                           0.83      2500\n",
      "   macro avg       0.83      0.83      0.83      2500\n",
      "weighted avg       0.83      0.83      0.83      2500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\anaconda3\\envs\\tfg\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass labels=['Blues' 'Classical' 'Country' 'Disco' 'Hiphop' 'Jazz' 'Metal' 'Pop'\n",
      " 'Reggae' 'Rock'] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def train_clasify():\n",
    "    \"\"\"\n",
    "    Method that train a RF Classifier and return a metrics report with\n",
    "        - Recall by label\n",
    "        - Precision by label\n",
    "        - F1-score by label\n",
    "        - Avg weighted recall\n",
    "        - Avg weighted precision\n",
    "        - Avg weighted f1-score\n",
    "        - Avg macro recall\n",
    "        - Avg macro recall\n",
    "        - Avg macro recall\n",
    "        - Avg total accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25)\n",
    "\n",
    "    print('Training Features Shape:', train_features.shape)\n",
    "    print('Training Labels Shape:', train_labels.shape)\n",
    "    print('Testing Features Shape:', test_features.shape)\n",
    "    print('Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_features)\n",
    "    train_features_scaled = scaler.transform(train_features)\n",
    "    test_features_scaled =  scaler.transform(test_features)\n",
    "    \n",
    "    print('Train Features Scaled Shape:', train_features_scaled.shape)\n",
    "    print('Testing Features Scaled Shape:', test_features_scaled.shape)\n",
    "    \n",
    "    # Instantiate model with 1000 decision trees\n",
    "    rf = RandomForestClassifier(n_estimators = 1000)\n",
    "    \n",
    "    scores = cross_val_score(rf, train_features_scaled, train_labels, cv=5)\n",
    "    \n",
    "    # Train the model on training data\n",
    "    rf.fit(train_features_scaled, train_labels)\n",
    "    \n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"Metrics info section\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    \n",
    "    print(\"Scores by fold\")\n",
    "    print(scores)\n",
    "    \n",
    "    importancia_predictores = pd.DataFrame({\n",
    "        'feature':\n",
    "        features.columns,\n",
    "        'importance':\n",
    "        rf.feature_importances_\n",
    "    })\n",
    "    print(\"Feature importance in the model\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(importancia_predictores.sort_values('importance', ascending=False))\n",
    "\n",
    "    predictions = rf.predict(test_features_scaled)\n",
    "\n",
    "\n",
    "    report = classification_report(test_labels, predictions, np.unique(labels))\n",
    "    return report\n",
    "\n",
    "report = train_clasify()\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import junix;\n",
    "junix.export_images(filepath= \"ClassificationTree.ipynb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-13692930308c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_size_inches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rf' is not defined"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(rf, test_features, test_labels)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(28.5, 28.5)\n",
    "plt.show()\n",
    "\n",
    "import junix;\n",
    "junix.export_images(filepath= \"ClassificationTree.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (7500, 13)\n",
      "Training Labels Shape: (7500,)\n",
      "Testing Features Shape: (2500, 13)\n",
      "Testing Labels Shape: (2500,)\n",
      "(9000,)\n",
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_317 (Dense)            (None, 512)               7168      \n",
      "_________________________________________________________________\n",
      "dense_318 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_319 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_320 (Dense)            (None, 64)                8256      \n",
      "=================================================================\n",
      "Total params: 179,648\n",
      "Trainable params: 179,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Score for fold 1: loss of 1.0702067613601685; accuracy of 85.6000006198883%\n",
      "(9000,)\n",
      "Model: \"sequential_82\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_321 (Dense)            (None, 512)               7168      \n",
      "_________________________________________________________________\n",
      "dense_322 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_323 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_324 (Dense)            (None, 64)                8256      \n",
      "=================================================================\n",
      "Total params: 179,648\n",
      "Trainable params: 179,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Score for fold 2: loss of 1.0502092838287354; accuracy of 86.79999709129333%\n",
      "(9000,)\n",
      "Model: \"sequential_83\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_325 (Dense)            (None, 512)               7168      \n",
      "_________________________________________________________________\n",
      "dense_326 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_327 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_328 (Dense)            (None, 64)                8256      \n",
      "=================================================================\n",
      "Total params: 179,648\n",
      "Trainable params: 179,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Score for fold 3: loss of 1.7506860494613647; accuracy of 80.9000015258789%\n",
      "(9000,)\n",
      "Model: \"sequential_84\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_329 (Dense)            (None, 512)               7168      \n",
      "_________________________________________________________________\n",
      "dense_330 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_331 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_332 (Dense)            (None, 64)                8256      \n",
      "=================================================================\n",
      "Total params: 179,648\n",
      "Trainable params: 179,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Score for fold 4: loss of 1.0255521535873413; accuracy of 88.09999823570251%\n",
      "(9000,)\n",
      "Model: \"sequential_85\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_333 (Dense)            (None, 512)               7168      \n",
      "_________________________________________________________________\n",
      "dense_334 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_335 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_336 (Dense)            (None, 64)                8256      \n",
      "=================================================================\n",
      "Total params: 179,648\n",
      "Trainable params: 179,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Score for fold 5: loss of 1.4825385808944702; accuracy of 85.6000006198883%\n",
      "(9000,)\n",
      "Model: \"sequential_86\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_337 (Dense)            (None, 512)               7168      \n",
      "_________________________________________________________________\n",
      "dense_338 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_339 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_340 (Dense)            (None, 64)                8256      \n",
      "=================================================================\n",
      "Total params: 179,648\n",
      "Trainable params: 179,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Score for fold 6: loss of 1.054179072380066; accuracy of 86.10000014305115%\n",
      "(9000,)\n",
      "Model: \"sequential_87\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_341 (Dense)            (None, 512)               7168      \n",
      "_________________________________________________________________\n",
      "dense_342 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_343 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_344 (Dense)            (None, 64)                8256      \n",
      "=================================================================\n",
      "Total params: 179,648\n",
      "Trainable params: 179,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Score for fold 7: loss of 0.9430317282676697; accuracy of 87.99999952316284%\n",
      "(9000,)\n",
      "Model: \"sequential_88\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_345 (Dense)            (None, 512)               7168      \n",
      "_________________________________________________________________\n",
      "dense_346 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_347 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_348 (Dense)            (None, 64)                8256      \n",
      "=================================================================\n",
      "Total params: 179,648\n",
      "Trainable params: 179,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Score for fold 8: loss of 0.970130980014801; accuracy of 87.00000047683716%\n",
      "(9000,)\n",
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_349 (Dense)            (None, 512)               7168      \n",
      "_________________________________________________________________\n",
      "dense_350 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_351 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_352 (Dense)            (None, 64)                8256      \n",
      "=================================================================\n",
      "Total params: 179,648\n",
      "Trainable params: 179,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Score for fold 9: loss of 0.9102073907852173; accuracy of 88.89999985694885%\n",
      "(9000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_90\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_353 (Dense)            (None, 512)               7168      \n",
      "_________________________________________________________________\n",
      "dense_354 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_355 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_356 (Dense)            (None, 64)                8256      \n",
      "=================================================================\n",
      "Total params: 179,648\n",
      "Trainable params: 179,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Score for fold 10: loss of 1.1807438135147095; accuracy of 83.20000171661377%\n"
     ]
    }
   ],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def train_ann_model(inputs, targets):\n",
    "    # Instantiate model with 1000 decision trees\n",
    "    model = keras.models.Sequential()\n",
    "#     model.add(keras.layers.LSTM(24, return_sequences=True, implementation=2))\n",
    "    model.add(keras.layers.Dense(512, activation='relu', input_shape=((inputs.shape[1],))))\n",
    "    model.add(keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(keras.layers.Dense(128, activation='relu'))\n",
    "#     model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # train\n",
    "    model_history = model.fit(inputs, targets,\n",
    "                                    epochs=512,\n",
    "                                    batch_size=64,\n",
    "#                                     validation_split=0.2,\n",
    "#                                     validation_data=(test_features, test_labels),\n",
    "                                    verbose=False)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "#     report = classification_report(test_labels, predictions, np.unique(labels))\n",
    "    \n",
    "    return model, model_history\n",
    "\n",
    "\n",
    "\n",
    "def train_clasify():\n",
    "    \"\"\"\n",
    "    Method that train a RF Classifier and return a metrics report with\n",
    "        - Recall by label\n",
    "        - Precision by label\n",
    "        - F1-score by label\n",
    "        - Avg weighted recall\n",
    "        - Avg weighted precision\n",
    "        - Avg weighted f1-score\n",
    "        - Avg macro recall\n",
    "        - Avg macro recall\n",
    "        - Avg macro recall\n",
    "        - Avg total accuracy\n",
    "    \"\"\"\n",
    "    # Encode \n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(labels)\n",
    "    labels_encoded = label_encoder.transform(labels)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels_encoded, test_size = 0.25)\n",
    "\n",
    "    print('Training Features Shape:', train_features.shape)\n",
    "    print('Training Labels Shape:', train_labels.shape)\n",
    "    print('Testing Features Shape:', test_features.shape)\n",
    "    print('Testing Labels Shape:', test_labels.shape)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_features)\n",
    "    train_features_scaled = scaler.transform(train_features)\n",
    "    test_features_scaled =  scaler.transform(test_features)\n",
    "    \n",
    "    # Merge inputs and targets\n",
    "    inputs = np.concatenate((train_features_scaled, test_features_scaled), axis=0)\n",
    "    targets = np.concatenate((train_labels, test_labels), axis=0)\n",
    "    \n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    fold_no = 1\n",
    "    \n",
    "    for train, test in kfold.split(inputs, targets):\n",
    "        print(train.shape)\n",
    "        model, history = train_ann_model(inputs[train], targets[train])\n",
    "        \n",
    "        scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "        print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "#         acc_per_fold.append(scores[1] * 100)\n",
    "#         loss_per_fold.append(scores[0])\n",
    "        \n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "    \n",
    "\n",
    "train_clasify()\n",
    "    \n",
    "# ann_model = train_clasify()\n",
    "\n",
    "# ann_model.history\n",
    "\n",
    "# plt.plot(ann_model.history['accuracy'])\n",
    "\n",
    "# matrix = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code was develop in order to implement corss validation obtaining more metrics that in sklearn version\n",
    "\"\"\"\n",
    "\n",
    "def get_metrics(report, tag, start_point):\n",
    "    \"\"\"\n",
    "    Method that extract metrics from the string table returned by the classification_report method from sklearn package\n",
    "    \"\"\"\n",
    "    avg_precision = report[report.index(tag)+start_point:report.index(tag)+start_point+4]\n",
    "    avg_recall = report[report.index(tag)+start_point+10:report.index(tag)+start_point+10+4]\n",
    "    avg_f1_score = report[report.index(tag)+start_point+20:report.index(tag)+start_point+20+4]\n",
    "    return avg_precision, avg_recall, avg_f1_score\n",
    "\n",
    "def get_metrics_avg(number_iterations):\n",
    "    \"\"\"\n",
    "    Method that make number_iterations training a RF classifier and return 2 dictionaires with metics\n",
    "    \n",
    "        - One with metrics by genre label\n",
    "        - The other with total metrics of all the genre labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initializing the by genre and total metrics arrays necessaries to catch the data\n",
    "    \n",
    "    blues_precision_array = np.arange(number_iterations).astype(np.float32)\n",
    "    blues_recall_array = np.arange(number_iterations).astype(np.float32)\n",
    "    blues_f1_score_array = np.arange(number_iterations).astype(np.float32)\n",
    "    \n",
    "    classical_precision_array = np.arange(number_iterations).astype(np.float32)\n",
    "    classical_recall_array = np.arange(number_iterations).astype(np.float32)\n",
    "    classical_f1_score_array = np.arange(number_iterations).astype(np.float32)\n",
    "    \n",
    "    country_precision_array = np.arange(number_iterations).astype(np.float32)\n",
    "    country_recall_array = np.arange(number_iterations).astype(np.float32)\n",
    "    country_f1_score_array = np.arange(number_iterations).astype(np.float32)\n",
    "    \n",
    "    disco_precision_array = np.arange(number_iterations).astype(np.float32)\n",
    "    disco_recall_array = np.arange(number_iterations).astype(np.float32)\n",
    "    disco_f1_score_array = np.arange(number_iterations).astype(np.float32)\n",
    "    \n",
    "    hiphop_precision_array = np.arange(number_iterations).astype(np.float32)\n",
    "    hiphop_recall_array = np.arange(number_iterations).astype(np.float32)\n",
    "    hiphop_f1_score_array = np.arange(number_iterations).astype(np.float32)\n",
    "    \n",
    "    jazz_precision_array = np.arange(number_iterations).astype(np.float32)\n",
    "    jazz_recall_array = np.arange(number_iterations).astype(np.float32)\n",
    "    jazz_f1_score_array = np.arange(number_iterations).astype(np.float32)\n",
    "    \n",
    "    metal_precision_array = np.arange(number_iterations).astype(np.float32)\n",
    "    metal_recall_array = np.arange(number_iterations).astype(np.float32)\n",
    "    metal_f1_score_array = np.arange(number_iterations).astype(np.float32)\n",
    "    \n",
    "    pop_precision_array = np.arange(number_iterations).astype(np.float32)\n",
    "    pop_recall_array = np.arange(number_iterations).astype(np.float32)\n",
    "    pop_f1_score_array = np.arange(number_iterations).astype(np.float32)\n",
    "    \n",
    "    reggae_precision_array = np.arange(number_iterations).astype(np.float32)\n",
    "    reggae_recall_array = np.arange(number_iterations).astype(np.float32)\n",
    "    reggae_f1_score_array = np.arange(number_iterations).astype(np.float32)\n",
    "    \n",
    "    rock_precision_array = np.arange(number_iterations).astype(np.float32)\n",
    "    rock_recall_array = np.arange(number_iterations).astype(np.float32)\n",
    "    rock_f1_score_array = np.arange(number_iterations).astype(np.float32)\n",
    "    \n",
    "    macro_avg_precision_array = np.arange(number_iterations).astype(np.float32)\n",
    "    macro_avg_recall_array = np.arange(number_iterations).astype(np.float32)\n",
    "    macro_avg_f1_score_array = np.arange(number_iterations).astype(np.float32)\n",
    "    \n",
    "    weighted_avg_precision_array = np.arange(number_iterations).astype(np.float32)\n",
    "    weighted_avg_recall_array = np.arange(number_iterations).astype(np.float32)\n",
    "    weighted_avg_f1_score_array = np.arange(number_iterations).astype(np.float32)\n",
    "    \n",
    "    accuracy_avg_array = np.arange(number_iterations).astype(np.float32)\n",
    "    \n",
    "    labels = ['Blues','Classical','Country','Disco','Hiphop','Jazz','Metal','Pop','Reggae','Rock']\n",
    "    \n",
    "    # Loop with the training RF classiffier itrerations. We fill the arrays metrics in each iteration     \n",
    "    \n",
    "    for i in range(0, number_iterations):\n",
    "        report = train_clasify()\n",
    "        \n",
    "        # Metrics by Genre\n",
    "        (precision, recall, f1_score) = get_metrics(report, \"Blues\", 12)\n",
    "        blues_precision_array[i] = precision\n",
    "        blues_recall_array[i] = recall\n",
    "        blues_f1_score_array[i] = f1_score\n",
    "        \n",
    "        (precision, recall, f1_score) = get_metrics(report, \"Classical\", 16)\n",
    "        classical_precision_array[i] = precision\n",
    "        classical_recall_array[i] = recall\n",
    "        classical_f1_score_array[i] = f1_score\n",
    "        \n",
    "        (precision, recall, f1_score) = get_metrics(report, \"Country\", 14)\n",
    "        country_precision_array[i] = precision\n",
    "        country_recall_array[i] = recall\n",
    "        country_f1_score_array[i] = f1_score\n",
    "        \n",
    "        (precision, recall, f1_score) = get_metrics(report, \"Disco\", 12)\n",
    "        disco_precision_array[i] = precision\n",
    "        disco_recall_array[i] = recall\n",
    "        disco_f1_score_array[i] = f1_score\n",
    "        \n",
    "        (precision, recall, f1_score) = get_metrics(report, \"Hiphop\", 13)\n",
    "        hiphop_precision_array[i] = precision\n",
    "        hiphop_recall_array[i] = recall\n",
    "        hiphop_f1_score_array[i] = f1_score\n",
    "        \n",
    "        (precision, recall, f1_score) = get_metrics(report, \"Jazz\", 11)\n",
    "        jazz_precision_array[i] = precision\n",
    "        jazz_recall_array[i] = recall\n",
    "        jazz_f1_score_array[i] = f1_score\n",
    "        \n",
    "        (precision, recall, f1_score) = get_metrics(report, \"Metal\", 12)\n",
    "        metal_precision_array[i] = precision\n",
    "        metal_recall_array[i] = recall\n",
    "        metal_f1_score_array[i] = f1_score\n",
    "        \n",
    "        (precision, recall, f1_score) = get_metrics(report, \"Pop\", 10)\n",
    "        pop_precision_array[i] = precision\n",
    "        pop_recall_array[i] = recall\n",
    "        pop_f1_score_array[i] = f1_score\n",
    "        \n",
    "        (precision, recall, f1_score) = get_metrics(report, \"Reggae\", 13)\n",
    "        reggae_precision_array[i] = precision\n",
    "        reggae_recall_array[i] = recall\n",
    "        reggae_f1_score_array[i] = f1_score\n",
    "        \n",
    "        (precision, recall, f1_score) = get_metrics(report, \"Rock\", 11)\n",
    "        rock_precision_array[i] = precision\n",
    "        rock_recall_array[i] = recall\n",
    "        rock_f1_score_array[i] = f1_score\n",
    "        \n",
    "        # General metrics         \n",
    "        \n",
    "        (precision, recall, f1_score) = get_metrics(report, \"accuracy\", 15)\n",
    "        accuracy_avg_array[i] = f1_score\n",
    "        \n",
    "        (precision, recall, f1_score) = get_metrics(report, \"weighted\", 19)\n",
    "        weighted_avg_precision_array[i] = precision\n",
    "        weighted_avg_recall_array[i] = recall\n",
    "        weighted_avg_f1_score_array[i] = f1_score\n",
    "        \n",
    "        (precision, recall, f1_score) = get_metrics(report, \"macro avg\", 16)\n",
    "        macro_avg_precision_array[i] = precision\n",
    "        macro_avg_recall_array[i] = recall\n",
    "        macro_avg_f1_score_array[i] = f1_score\n",
    "        \n",
    "    # Now we fill the arrays that we are going to use to create the final dictionaries     \n",
    "    \n",
    "    precision_array = []\n",
    "    recall_array = []\n",
    "    f1_score_array = []\n",
    "    \n",
    "    precision_array.append(blues_precision_array.mean())\n",
    "    recall_array.append(blues_recall_array.mean())\n",
    "    f1_score_array.append(blues_f1_score_array.mean())\n",
    "    \n",
    "    precision_array.append(classical_precision_array.mean())\n",
    "    recall_array.append(classical_recall_array.mean())\n",
    "    f1_score_array.append(classical_f1_score_array.mean())\n",
    "    \n",
    "    precision_array.append(country_precision_array.mean())\n",
    "    recall_array.append(country_recall_array.mean())\n",
    "    f1_score_array.append(country_f1_score_array.mean())\n",
    "    \n",
    "    precision_array.append(disco_precision_array.mean())\n",
    "    recall_array.append(disco_recall_array.mean())\n",
    "    f1_score_array.append(disco_f1_score_array.mean())\n",
    "    \n",
    "    precision_array.append(hiphop_precision_array.mean())\n",
    "    recall_array.append(hiphop_recall_array.mean())\n",
    "    f1_score_array.append(hiphop_f1_score_array.mean())\n",
    "    \n",
    "    precision_array.append(jazz_precision_array.mean())\n",
    "    recall_array.append(jazz_recall_array.mean())\n",
    "    f1_score_array.append(jazz_f1_score_array.mean())\n",
    "    \n",
    "    precision_array.append(metal_precision_array.mean())\n",
    "    recall_array.append(metal_recall_array.mean())\n",
    "    f1_score_array.append(metal_f1_score_array.mean())\n",
    "    \n",
    "    precision_array.append(pop_precision_array.mean())\n",
    "    recall_array.append(pop_recall_array.mean())\n",
    "    f1_score_array.append(pop_f1_score_array.mean())\n",
    "    \n",
    "    precision_array.append(reggae_precision_array.mean())\n",
    "    recall_array.append(reggae_recall_array.mean())\n",
    "    f1_score_array.append(reggae_f1_score_array.mean())\n",
    "    \n",
    "    precision_array.append(rock_precision_array.mean())\n",
    "    recall_array.append(rock_recall_array.mean())\n",
    "    f1_score_array.append(rock_f1_score_array.mean())\n",
    "    \n",
    "    # Dictionaries creation     \n",
    "    \n",
    "    by_label_metrics_dict = {\n",
    "        \n",
    "        'Genre': labels,\n",
    "        'avg_precision': precision_array,\n",
    "        'avg_recall': recall_array,\n",
    "        'avg_f1_score': f1_score_array,\n",
    "        \n",
    "    }\n",
    "    \n",
    "    total_metrics_dict = {\n",
    "        \n",
    "        'avg_accuracy': accuracy_avg_array.mean(),\n",
    "        \n",
    "        'weighted_avg_precision': weighted_avg_precision_array.mean(),\n",
    "        'weighted_avg_recall': weighted_avg_recall_array.mean(),\n",
    "        'weighted_avg_f1_score': weighted_avg_f1_score_array.mean(),\n",
    "        \n",
    "        'macro_avg_precision': macro_avg_precision_array.mean(),\n",
    "        'macro_avg_recall': macro_avg_recall_array.mean(),\n",
    "        'macro_avg_f1_score': macro_avg_f1_score_array.mean(),\n",
    "        \n",
    "    }\n",
    "\n",
    "    return by_label_metrics_dict, total_metrics_dict\n",
    "\n",
    "\n",
    "def export_genre_result_dict_to_csv(by_label_metrics_dict, total_metrics_dict):\n",
    "    \"\"\"\n",
    "    Method that export two dictionaries to csv files using pandas dataframes\n",
    "    \n",
    "        - One with metrics by genre label\n",
    "        - The other with total metrics of all the genre labels\n",
    "    \"\"\"\n",
    "    by_label_metrics_dataframe_columns_tags = ['Genre', 'avg_precision', 'avg_recall', 'avg_f1_score']\n",
    "    total_metrics_dataframe_columns_tags = ['avg_accuracy', \n",
    "                                            'weighted_avg_precision', 'weighted_avg_recall', 'weighted_avg_f1_score',\n",
    "                                            'macro_avg_precision', 'macro_avg_recall', 'macro_avg_f1_score'\n",
    "                                           ]\n",
    "\n",
    "    df_by_label = pd.DataFrame(by_label_metrics_dict, columns = by_label_metrics_dataframe_columns_tags)\n",
    "    df_total = pd.DataFrame(total_metrics_dict, columns = total_metrics_dataframe_columns_tags, index=[0])\n",
    "\n",
    "    df_by_label.to_csv('RF_Standarized_byLabel.csv')\n",
    "    df_total.to_csv('RF_Standarized_total.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
